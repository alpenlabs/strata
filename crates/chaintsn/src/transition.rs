//! Top-level CL state transition logic.  This is largely stubbed off now, but
//! we'll replace components with real implementations as we go along.
#![allow(unused)]

use std::{cmp::max, collections::HashMap};

use bitcoin::{OutPoint, Transaction};
use rand_core::{RngCore, SeedableRng};
use strata_primitives::{
    batch::SignedCheckpoint,
    epoch::EpochCommitment,
    l1::{BitcoinAmount, DepositInfo, L1BlockManifest, L1TxRef, OutputRef, ProtocolOperation},
    params::RollupParams,
};
use strata_state::{
    block::L1Segment,
    bridge_ops::{DepositIntent, WithdrawalIntent},
    bridge_state::{DepositState, DispatchCommand, WithdrawOutput},
    exec_env::ExecEnvState,
    exec_update::{self, construct_ops_from_deposit_intents, ELDepositData, Op},
    prelude::*,
    state_op::StateCache,
    state_queue,
};

use crate::{
    errors::TsnError,
    macros::*,
    slot_rng::{self, SlotRng},
};

/// Processes a block, making writes into the provided state cache.
///
/// The cache will eventually be written to disk.  This does not check the
/// block's credentials, it plays out all the updates a block makes to the
/// chain, but it will abort if there are any semantic issues that
/// don't make sense.
///
/// This operates on a state cache that's expected to be empty, may panic if
/// changes have been made, although this is not guaranteed.  Does not check the
/// `state_root` in the header for correctness, so that can be unset so it can
/// be use during block assembly.
pub fn process_block(
    state: &mut StateCache,
    header: &impl L2Header,
    body: &L2BlockBody,
    params: &RollupParams,
) -> Result<(), TsnError> {
    // We want to fail quickly here because otherwise we don't know what's
    // happening.
    if !state.is_empty() {
        panic!("transition: state cache not fresh");
    }

    let mut rng = compute_init_slot_rng(state);

    // Update basic bookkeeping.
    state.set_cur_header(header);

    // Go through each stage and play out the operations it has.
    let new_epoch = process_l1_view_update(state, body.l1_segment(), params)?;
    let ready_withdrawals = process_execution_update(state, body.exec_segment().update())?;
    process_deposit_updates(state, ready_withdrawals, &mut rng, params)?;

    // If we checked in with L1, then advance the epoch.
    if new_epoch {
        advance_epoch_tracking(state, header)?;
    }

    Ok(())
}

/// Constructs the slot RNG used for processing the block.
///
/// This is meant to be independent of the block's body so that it's less
/// manipulatable.  Eventually we want to switch to a randao-ish scheme, but
/// let's not get ahead of ourselves.
fn compute_init_slot_rng(state: &StateCache) -> SlotRng {
    // Just take the last block's slot.
    let blkid_buf = *state.state().chain_tip_blkid().as_ref();
    SlotRng::from_seed(blkid_buf)
}

/// Update our view of the L1 state, playing out downstream changes from that.
///
/// Returns if there was an update processed.
fn process_l1_view_update(
    state: &mut StateCache,
    l1seg: &L1Segment,
    params: &RollupParams,
) -> Result<bool, TsnError> {
    let l1v = state.state().l1_view();

    // Accept new blocks.
    // FIXME this should actually check PoW, it just does it based on block heights
    if !l1seg.new_manifests().is_empty() {
        let cur_safe_height = l1v.safe_height();

        // Validate the new blocks actually extend the tip.  This is what we have to tweak to make
        // more complicated to check the PoW.
        let new_tip_height = cur_safe_height + l1seg.new_manifests().len() as u64;
        if new_tip_height <= l1v.safe_height() {
            return Err(TsnError::L1SegNotExtend);
        }

        // First check that the blocks are correct.
        check_chain_integrity(
            cur_safe_height,
            l1v.safe_blkid(),
            l1seg.new_height(),
            l1seg.new_manifests(),
        )?;

        // Go through each manifest and process it.
        for (off, b) in l1seg.new_manifests().iter().enumerate() {
            let height = cur_safe_height + off as u64 + 1;
            process_l1_block(state, b)?;
            state.update_safe_block(height, b.record().clone());
        }

        /*
        let first_new_block_height = new_tip_height - l1seg.new_payloads().len() as u64 + 1;
        let implied_pivot_height = first_new_block_height - 1;
        let next_exp_height = l1v.next_expected_height();
        let cur_safe_height = l1v.safe_height();

        // Now make sure that the block hashes all connect up sensibly.
        let pivot_idx = implied_pivot_height;
        let pivot_blkid = l1v
            .maturation_queue()
            .get_absolute(pivot_idx)
            .map(|b| b.blkid())
            .unwrap_or_else(|| l1v.safe_block().blkid());

        // Okay now that we've figured that out, let's actually how to actually do the reorg.
        if pivot_idx > params.horizon_l1_height && pivot_idx < next_exp_height {
            state.revert_l1_view_to(pivot_idx);
        }

        let maturation_threshold = params.l1_reorg_safe_depth as u64;

        for e in l1seg.new_payloads() {
            let ment = L1MaturationEntry::from(e.clone());
            state.apply_l1_block_entry(ment.clone());
        }

        let new_safe_height = max(
            new_tip_height.saturating_sub(maturation_threshold),
            cur_safe_height,
        );

        for idx in (cur_safe_height + 1..=new_safe_height) {
            state.mature_l1_block(idx);
        }*/

        Ok(true)
    } else {
        Ok(false)
    }
}

fn process_l1_block(state: &mut StateCache, block_mf: &L1BlockManifest) -> Result<(), TsnError> {
    // Just iterate through every tx's operation and call out to the handlers for that.
    for tx in block_mf.txs() {
        for op in tx.protocol_ops() {
            match &op {
                ProtocolOperation::Checkpoint(ckpt) => {
                    process_l1_checkpoint(state, block_mf, ckpt)?;
                }

                ProtocolOperation::Deposit(info) => {
                    process_l1_deposit(state, block_mf, info)?;
                }

                // Other operations we don't do anything with for now.
                _ => {}
            }
        }
    }

    Ok(())
}

fn process_l1_checkpoint(
    state: &mut StateCache,
    src_block_mf: &L1BlockManifest,
    signed_ckpt: &SignedCheckpoint,
) -> Result<(), TsnError> {
    // TODO verify signature?  it should already have been validated but it
    // doesn't hurt to do it again (just costs)
    // TODO should we verify the proof here too?  the L1 scan proof probably
    // should have done it, but it wouldn't be excessively complicated to
    // re-verify it, we should formally define the answers to these questions

    let ckpt = signed_ckpt.checkpoint(); // inner data

    // Copy the epoch commitment and make it finalized.
    let old_fin_epoch = state.state().finalized_epoch();
    let new_fin_epoch = ckpt.batch_info().get_epoch_commitment();

    // TODO go through and do whatever stuff we need to do now that's finalized

    state.set_finalized_epoch(new_fin_epoch);

    Ok(())
}

fn process_l1_deposit(
    state: &mut StateCache,
    src_block_mf: &L1BlockManifest,
    info: &DepositInfo,
) -> Result<(), TsnError> {
    let outpoint = info.outpoint;

    // Create the deposit entry to track it on the bridge side.
    //
    // Right now all operators sign all deposits, take them all.
    let all_operators = state.state().operator_table().indices().collect::<_>();
    state.insert_deposit_entry(outpoint, info.amt, all_operators);

    // Insert an intent to credit the destination with it.
    let deposit_intent = DepositIntent::new(info.amt, info.address.clone());
    state.insert_deposit_intent(0, deposit_intent);

    // Logging so we know if it got there.
    debug!(?outpoint, "handled deposit");

    Ok(())
}

/// Advances the epoch bookkeeping, using the provided header as the terminal.
fn advance_epoch_tracking(state: &mut StateCache, header: &impl L2Header) -> Result<(), TsnError> {
    let cur_epoch = state.state().cur_epoch();
    let this_epoch = EpochCommitment::new(cur_epoch, header.blockidx(), header.get_blockid());
    state.set_prev_epoch(this_epoch);
    state.set_cur_epoch(cur_epoch + 1);
    Ok(())
}

/// Checks the attested block IDs and parent blkid connections in new blocks.
// TODO unit tests
fn check_chain_integrity(
    cur_safe_height: u64,
    cur_safe_blkid: &L1BlockId,
    new_height: u64,
    new_blocks: &[L1BlockManifest],
) -> Result<(), TsnError> {
    // Check that the heights match.
    if new_height != cur_safe_height + new_blocks.len() as u64 {
        // This is basically right for both cases.
        return Err(TsnError::SkippedBlock);
    }

    // Iterate over all the blocks in the new list and make sure they match.
    for (i, e) in new_blocks.iter().enumerate() {
        let height = cur_safe_height + i as u64;

        // Make sure the hash matches.
        let computed_id = L1BlockId::compute_from_header_buf(e.header());
        let attested_id = e.record().blkid();
        if computed_id != *attested_id {
            return Err(TsnError::L1BlockIdMismatch(
                height,
                *attested_id,
                computed_id,
            ));
        }

        // Make sure matches parent.
        // TODO FIXME I think my impl for parent_blkid is incorrect, fix this later
        /*let blk_parent = e.record().parent_blkid();
        if i == 0 {
            if blk_parent != *pivot_blkid {
                return Err(TsnError::L1BlockParentMismatch(h, blk_parent, *pivot_blkid));
            }
        } else {
            let parent_payload = &new_blocks[i - 1];
            let parent_id = parent_payload.record().blkid();
            if blk_parent != *parent_id {
                return Err(TsnError::L1BlockParentMismatch(h, blk_parent, *parent_id));
            }
        }*/
    }

    Ok(())
}

/// Process an execution update, to change an exec env state.
///
/// This is meant to be kinda generic so we can reuse it across multiple exec
/// envs if we decide to go in that direction.
///
/// Note: As this is currently written, it assumes that the withdrawal state is
/// correct, which means that the sequencer kinda just gets to decide what the
/// withdrawals are.  Fortunately this is fine for now, since we're relying on
/// operators to also check all the parts of the state transition themselves,
/// including the EL payload itself.
///
/// Note: Currently this returns a ref to the withdrawal intents passed in the
/// exec update, but really it might need to be a ref into the state cache.
/// This will probably be substantially refactored in the future though.
fn process_execution_update<'u>(
    state: &mut StateCache,
    update: &'u exec_update::ExecUpdate,
) -> Result<&'u [WithdrawalIntent], TsnError> {
    // for all the ops, corresponding to DepositIntent, remove those DepositIntent the ExecEnvState
    let applied_ops = update.input().applied_ops();

    let applied_deposit_intent_idx = applied_ops
        .iter()
        .filter_map(|op| match op {
            Op::Deposit(deposit) => Some(deposit.intent_idx()),
            _ => None,
        })
        .max();

    if let Some(intent_idx) = applied_deposit_intent_idx {
        state.consume_deposit_intent(intent_idx);
    }

    Ok(update.output().withdrawals())
}

/// Iterates over the deposits table, making updates where needed.
///
/// Includes:
/// * Processes L1 withdrawals that are safe to dispatch to specific deposits.
/// * Reassigns deposits that have passed their deadling to new operators.
/// * Cleans up deposits that have been handled and can be removed.
fn process_deposit_updates(
    state: &mut StateCache,
    ready_withdrawals: &[WithdrawalIntent],
    rng: &mut SlotRng,
    params: &RollupParams,
) -> Result<(), TsnError> {
    // TODO make this capable of handling multiple denominations, have to decide
    // how those get represented first though

    let num_deposit_ents = state.state().deposits_table().len();

    // This determines how long we'll keep trying to service a withdrawal before
    // updating it or doing something else with it.  This is also what we use
    // when we decide to reset an assignment.
    let cur_block_height = state.state().l1_view().safe_height();
    let new_exec_height = cur_block_height as u32 + params.dispatch_assignment_dur;

    // Sequence in which we assign the operators to the deposits.  This is kinda
    // shitty because it might not account for available funds but it works for
    // devnet.
    //
    // TODO make this actually pick operators and not always use the first one,
    // this will be easier when we have operators able to reason about the funds
    // they have available on L1 on the rollup chain, perhaps a nomination queue
    //
    // TODO the way we pick assignees right now is a bit weird, we compute a
    // possible list for all possible new assignees, but then if we encounter a
    // deposit that needs reassignment we pick it directly at the time we need
    // it instead of taking it out of the precomputed table, this seems fine and
    // minimizes total calls to the RNG but feels odd since the order we pick the
    // numbers isn't the same as the order we've assigned
    let num_operators = state.state().operator_table().len();

    // A bit of a sanity check, but also idk it's weird to not have this.
    if num_operators == 0 {
        return Err(TsnError::NoOperators);
    }

    let ops_seq = (0..ready_withdrawals.len())
        .map(|_| next_rand_op_pos(rng, num_operators))
        .collect::<Vec<_>>();

    let mut next_intent_to_assign = 0;
    let mut deposit_idxs_to_remove = Vec::new();

    for deposit_entry_idx in 0..num_deposit_ents {
        let ent = state
            .state()
            .deposits_table()
            .get_entry_at_pos(deposit_entry_idx)
            .expect("chaintsn: inconsistent state");
        let deposit_idx = ent.idx();

        let have_ready_intent = next_intent_to_assign < ready_withdrawals.len();

        match ent.deposit_state() {
            DepositState::Created(_) => {
                // TODO I think we can remove this state
            }

            DepositState::Accepted => {
                // If we have an intent to assign, we can dispatch it to this deposit.
                if have_ready_intent {
                    let intent = &ready_withdrawals[next_intent_to_assign];
                    let op_idx = ops_seq[next_intent_to_assign % ops_seq.len()];

                    let outp = WithdrawOutput::new(intent.destination().clone(), *intent.amt());
                    let cmd = DispatchCommand::new(vec![outp]);
                    state.assign_withdrawal_command(
                        deposit_idx,
                        op_idx,
                        cmd,
                        new_exec_height as u64,
                    );

                    next_intent_to_assign += 1;
                }
            }

            DepositState::Dispatched(dstate) => {
                // Check if the deposit is past the threshold.
                if cur_block_height >= dstate.exec_deadline() {
                    // Pick the next assignee, if there are any.
                    let new_op_pos = if num_operators > 1 {
                        // Compute a random offset from 1 to (num_operators - 1),
                        // ensuring we pick a different operator than the current one.
                        let offset = 1 + (rng.next_u32() % (num_operators - 1));
                        (dstate.assignee() + offset) % num_operators
                    } else {
                        // If there is only a single operator, we remain with the current assignee.
                        dstate.assignee()
                    };

                    // Convert their position in the table to their global index.
                    let op_idx = state
                        .state()
                        .operator_table()
                        .get_entry_at_pos(new_op_pos)
                        .expect("chaintsn: inconsistent state")
                        .idx();

                    state.reset_deposit_assignee(deposit_idx, op_idx, new_exec_height as u64);
                }
            }

            DepositState::Executed => {
                deposit_idxs_to_remove.push(deposit_idx);
            }
        }
    }

    // Sanity check.  For devnet this should never fail since we should never be
    // able to withdraw more than was deposited, so we should never run out of
    // deposits to assign withdrawals to.
    if next_intent_to_assign != ready_withdrawals.len() {
        return Err(TsnError::InsufficientDepositsForIntents);
    }

    // TODO remove stale deposit idxs

    Ok(())
}

/// Wrapper to safely select a random operator index using wide reduction
/// This will return a deterministically-random index in the range `[0, num)`
fn next_rand_op_pos(rng: &mut SlotRng, num: u32) -> u32 {
    // This won't meaningfully truncate since `num` is `u32`
    (rng.next_u64() % (num as u64)) as u32
}

#[cfg(test)]
mod tests {
    use rand_core::SeedableRng;
    use strata_primitives::{
        buf::Buf32,
        l1::{
            BitcoinAmount, DepositInfo, DepositUpdateTx, L1BlockManifest, L1HeaderRecord, L1Tx,
            ProtocolOperation,
        },
        l2::L2BlockId,
        params::OperatorConfig,
    };
    use strata_state::{
        block::{ExecSegment, L1Segment, L2BlockBody},
        bridge_state::OperatorTable,
        chain_state::Chainstate,
        exec_env::ExecEnvState,
        exec_update::{ExecUpdate, UpdateInput, UpdateOutput},
        genesis::GenesisStateData,
        header::{L2BlockHeader, L2Header},
        l1::L1ViewState,
        state_op::StateCache,
    };
    use strata_test_utils::{l2::gen_params, ArbitraryGenerator};

    use super::{next_rand_op_pos, process_block};
    use crate::{slot_rng::SlotRng, transition::process_l1_view_update};

    #[test]
    // Confirm that operator index sampling is deterministic and in bounds
    fn deterministic_index_sampling() {
        let num = 123;
        let mut rng = SlotRng::from_seed([1u8; 32]);
        let mut same_rng = SlotRng::from_seed([1u8; 32]);

        let index = next_rand_op_pos(&mut rng, num);
        let same_index = next_rand_op_pos(&mut same_rng, num);

        assert_eq!(index, same_index);
        assert!(index < num);
    }

    #[test]
    fn test_process_l1_view_update_with_empty_payload() {
        let chs: Chainstate = ArbitraryGenerator::new().generate();
        let params = gen_params();

        let mut state_cache = StateCache::new(chs.clone());

        // Empty L1Segment payloads
        let l1_segment = L1Segment::new_empty(chs.l1_view().safe_height());

        // let previous_maturation_queue =
        // Process the empty payload
        let result = process_l1_view_update(&mut state_cache, &l1_segment, params.rollup());
        assert_eq!(state_cache.state(), &chs);
        assert!(result.is_ok());
    }
}
